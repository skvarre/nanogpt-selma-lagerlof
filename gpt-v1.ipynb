{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import wandb\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab run\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# !pip install wandb\n",
    "# !wandb login\n",
    "# content = \"/content/drive/MyDrive/content.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Hyperparameters\n",
    "block_size = 16\n",
    "batch_size = 8\n",
    "max_iterations = 20\n",
    "lr = 3e-4\n",
    "eval_iterations = 100\n",
    "dropout = 0.2\n",
    "num_embd = 384 # Total dimensions to capture in the embedding\n",
    "num_layers = 8\n",
    "num_head = 8\n",
    "split_factor = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"content.txt\" if 'content' not in globals() else globals()['content']\n",
    "with open(content, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        text = f.read()\n",
    "\n",
    "split = int(len(text) * split_factor) \n",
    "train_data = text[:split]\n",
    "test_data = text[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (504295 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# --- GPT-SW3-TOKENIZER --- \n",
    "\n",
    "# Try to use string level tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "import sentencepiece\n",
    "\n",
    "model_name = \"AI-Sweden-Models/gpt-sw3-126m\"\n",
    "# Import AI Sweden's tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "encode = tokenizer.encode\n",
    "decode = tokenizer.decode\n",
    "\n",
    "# Vocabulary size \n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Encode\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(test_data)\n",
    "# print(f\"Train data has {len(train_ids)} tokens\")\n",
    "# print(f\"Val data has {len(val_ids)} tokens\")\n",
    "\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "\n",
    "train_ids.tofile(os.path.join(os.getcwd(), 'train.bin'))\n",
    "val_ids.tofile(os.path.join(os.getcwd(), 'val.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- CHAR LEVEL TOKENIZER --- ##\n",
    "characters = sorted(list(set(text)))\n",
    "vocab_size = len(characters)\n",
    "\n",
    "# Creating a mapping from unique characters to indices\n",
    "string_to_int = { char:index for index, char in enumerate(characters)}\n",
    "int_to_string = { index:char for index, char in enumerate(characters)}\n",
    "\n",
    "# Encoding the text\n",
    "encode = lambda s : [string_to_int[c] for c in s]\n",
    "decode = lambda c : ''.join([int_to_string[i] for i in c])\n",
    "\n",
    "# Turn into tensors \n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
    "test_data = np.memmap('val.bin', dtype=np.uint16, mode='r')\n",
    "\n",
    "# train_data = data[:split].to(device)\n",
    "# test_data = data[split:].to(device)\n",
    "\n",
    "def get_batch(split: str):\n",
    "    \"\"\"Returns a random batch from the data\"\"\"\n",
    "    data = train_data if split == \"train\" else test_data\n",
    "    start_index = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # print(start_index)\n",
    "    x = torch.stack([torch.from_numpy((data[idx:idx+block_size]).astype(np.int64)) for idx in start_index])\n",
    "    y = torch.stack([torch.from_numpy((data[idx+1:idx+block_size+1]).astype(np.int64)) for idx in start_index])\n",
    "    # x = torch.stack([data[idx:idx+block_size] for idx in start_index])\n",
    "    # y = torch.stack([data[idx+1:idx+block_size+1] for idx in start_index])\n",
    "    x, y = x.to(device), y.to(device) # Make sure data is on the right device\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model: nn.Module):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iterations)\n",
    "            for k in range(eval_iterations):\n",
    "                X, Y = get_batch(split)\n",
    "                logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention\"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(num_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(num_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(num_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape (Batch, time-step, channels)\n",
    "        # Output shape: (Batch, time-step, head size)\n",
    "        B, T, C = x.shape\n",
    "        # Calling the transformations, but different ones for each\n",
    "        k = self.key(x) # (B, T, head size)\n",
    "        q = self.query(x) # (B, T, head size)\n",
    "        # Compute attention scores\n",
    "        weights = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Mask out upper triangular \n",
    "        # Apply softmax along the last dimension \n",
    "        weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
    "        weights = self.dropout(weights)\n",
    "        # Apply attention\n",
    "        v = self.value(x) # (B, T, head size)\n",
    "        out = weights @ v # (B, T, head size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(in_features=head_size * num_heads, out_features=num_embd)\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) where F = head_size * num_heads\n",
    "        out = self.dropout(self.proj(out)) # (B, T, E)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple feed forward with linear layers and ReLU activations\"\"\"\n",
    "    def __init__(self, num_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=num_embd, out_features=4*num_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=4*num_embd, out_features=num_embd),\n",
    "            nn.Dropout(dropout) # Dropout to prevent overfitting\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Implementation of a Transformer block\"\"\"\n",
    "    def __init__(self, num_embd, num_head):\n",
    "        super().__init__()\n",
    "        head_size = num_embd // num_head \n",
    "        self.self_attention = MultiHeadAttention(num_head, head_size)\n",
    "        self.feed_forward = FeedForward(num_embd)\n",
    "        self.ln1 = nn.LayerNorm(num_embd)\n",
    "        self.ln2 = nn.LayerNorm(num_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.self_attention(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.feed_forward(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, num_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, num_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(num_embd, num_head=num_head) for _ in range(num_layers)])\n",
    "        self.ln_f = nn.LayerNorm(num_embd) # Layer normalization\n",
    "        self.lm_head = nn.Linear(num_embd, vocab_size) # Language modeling head, i.e. the weights to the logits\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None): \n",
    "        B, T = index.shape\n",
    "        # Feed the sequence through the transformer blocks\n",
    "        token_embd = self.token_embedding_table(index)\n",
    "        position_embd = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        x = token_embd + position_embd # (B, T, C)\n",
    "        x = self.blocks(x) # (B, T, C)\n",
    "        x = self.ln_f(x) # (B, T, C)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape # batch_size, time, vocab_size\n",
    "            logits = logits.view(B*T, C) # Flatten the batch and time dimensions\n",
    "            targets = targets.view(B*T) # Flatten the batch and time dimensions \n",
    "            loss = F.cross_entropy(logits, targets) # Calculate the loss \n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # Generate new content\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            index_cond = index[:, -block_size:] # (B, T)\n",
    "            # Make predictions\n",
    "            logits, loss = self.forward(index_cond)\n",
    "            # Get the last time step\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            # Turn into probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # Sample from the distribution or take the most likely\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # Append to the sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = GPTModel(vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log to wandb\n",
    "run = wandb.init(\n",
    "          project = \"nanoGPT\",\n",
    "          config = {\n",
    "               \"learning_rate\": lr,\n",
    "               \"dataset\": \"selma-lagerlof-greatest-hits-2k24\",\n",
    "               \"epochs\": max_iterations,\n",
    "               \"number_of_heads\": num_head,\n",
    "               \"number_of_layers\": num_layers,\n",
    "               \"block_size\": block_size,\n",
    "               \"batch_size\": batch_size,\n",
    "               \"dropout\": dropout,\n",
    "               \"embedding_size\": num_embd,\n",
    "               \"split_factor\": split_factor,\n",
    "               \"no_parameters\": sum(p.numel() for p in model_0.parameters() if p.requires_grad)\n",
    "          }\n",
    ")\n",
    "\n",
    "text_table = wandb.Table(columns=[\"epoch\", \"val/loss\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model_0.parameters(), lr=lr)\n",
    "\n",
    "for iter in tqdm(range(max_iterations)):\n",
    "    if iter % eval_iterations == 0:\n",
    "        losses = estimate_loss(model_0)\n",
    "        \n",
    "        # Log train and val loss to wandb\n",
    "        run.log({\"val/loss\": losses[\"val\"]}, step=iter)\n",
    "        \n",
    "        # Log text prompt \"Det\" to wandb\n",
    "        context = torch.tensor(encode(\"Det\"), dtype=torch.long, device=device).unsqueeze(0)\n",
    "        text_table.add_data(iter, losses[\"val\"], decode(model_0.generate(context, max_new_tokens=50)[0].tolist()))\n",
    "        \n",
    "        print(f'Step: {iter} | Train Loss: {losses[\"train\"]:.4f} | Val Loss: {losses[\"val\"]:.4f}')\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # Calculate the loss \n",
    "    logits, loss = model_0(xb, yb)\n",
    "    # Log the loss to wandb\n",
    "    run.log({\"train/loss\": loss.item()}, step=iter)\n",
    "    optimizer.zero_grad(set_to_none=True) # Clear the gradients\n",
    "    loss.backward() # Calculate the gradients\n",
    "    optimizer.step() # Update the weights\n",
    "\n",
    "print(loss.item())\n",
    "run.log({\"training_samples\" : text_table})\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tjena Nisse! Hur är läget?\"\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
    "# context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "# generated_chars = decode(model_0.generate(context, max_new_tokens=100)[0].tolist())\n",
    "# print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
